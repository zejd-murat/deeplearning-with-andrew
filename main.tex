\documentclass[11pt,a4paper]{article}

% ============================================================================
% PACKAGE IMPORTS
% ============================================================================

% Font and encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}

% Language and typography
\usepackage[english]{babel}
\usepackage{csquotes}

% Page layout and geometry
\usepackage[margin=1in, paperheight=18in, paperwidth=8.5in]{geometry}
\usepackage{setspace}
\usepackage{parskip}

% Mathematics
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}

% Graphics and figures
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}

% Colors and styling
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{mdframed}
\usepackage{framed}

% Tables
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}

% Lists and enumeration
\usepackage{enumitem}

% Code and algorithms
\usepackage{listings}
\usepackage[ruled,vlined]{algorithm2e}

% References and links
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{url}

% Headers and footers
\usepackage{fancyhdr}
\usepackage{lastpage}

% Bibliography
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}

% ============================================================================
% COLOR DEFINITIONS
% ============================================================================

\definecolor{primaryblue}{RGB}{41, 128, 185}
\definecolor{secondaryblue}{RGB}{52, 152, 219}
\definecolor{darkblue}{RGB}{44, 62, 80}
\definecolor{lightblue}{RGB}{174, 214, 241}
\definecolor{accentorange}{RGB}{230, 126, 34}
\definecolor{accentgreen}{RGB}{46, 125, 50}
\definecolor{accentred}{RGB}{231, 76, 60}
\definecolor{lightgray}{RGB}{236, 240, 241}
\definecolor{darkgray}{RGB}{84, 110, 122}
\definecolor{codegray}{RGB}{248, 249, 250}

% ============================================================================
% HYPERREF SETUP
% ============================================================================

\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    filecolor=primaryblue,
    urlcolor=primaryblue,
    citecolor=primaryblue,
    bookmarksopen=true,
    bookmarksnumbered=true,
    pdfstartview={FitH},
    pdfauthor={Zejd Murat},
    pdftitle={Deep Learning with Andrew Ng - Study Notes},
    pdfsubject={Deep Learning},
    pdfkeywords={Deep Learning, Machine Learning, Neural Networks, Andrew Ng}
}

% ============================================================================
% CUSTOM ENVIRONMENTS
% ============================================================================

% Theorem-like environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{note}{Note}[section]

% Custom colored boxes
\tcbuselibrary{theorems}

% Key Concept Box
\newtcolorbox{keyconcept}{
    colback=lightblue,
    colframe=primaryblue,
    fonttitle=\bfseries,
    title=Key Concept,
    rounded corners,
    boxrule=1pt,
    left=8pt,
    right=8pt,
    top=8pt,
    bottom=8pt
}

% Important Formula Box
\newtcolorbox{formula}{
    colback=lightgray,
    colframe=darkgray,
    fonttitle=\bfseries,
    title=Important Formula,
    rounded corners,
    boxrule=1pt,
    left=8pt,
    right=8pt,
    top=8pt,
    bottom=8pt
}

% Warning/Attention Box
\newtcolorbox{attention}{
    colback=accentorange!10,
    colframe=accentorange,
    fonttitle=\bfseries,
    title=Attention,
    rounded corners,
    boxrule=1pt,
    left=8pt,
    right=8pt,
    top=8pt,
    bottom=8pt
}

% Intuition Box
\newtcolorbox{intuition}{
    colback=accentgreen!10,
    colframe=accentgreen,
    fonttitle=\bfseries,
    title=Intuition,
    rounded corners,
    boxrule=1pt,
    left=8pt,
    right=8pt,
    top=8pt,
    bottom=8pt
}

% Follow-up Box
\newtcolorbox{followup}{
    colback=secondaryblue!10,
    colframe=secondaryblue,
    fonttitle=\bfseries,
    title=Follow-up,
    rounded corners,
    boxrule=1pt,
    left=8pt,
    right=8pt,
    top=8pt,
    bottom=8pt
}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================

% Common math operators
\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\relu}{ReLU}
\DeclareMathOperator{\mytanh}{tanh}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\trace}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\mybias}{Bias}

% Vectors and matrices
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\trans}[1]{#1^{\top}}
\newcommand{\inv}[1]{#1^{-1}}

% Probability and statistics
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\given}{\mid}
\newcommand{\normal}[2]{\mathcal{N}\left(#1, #2\right)}

% Common sets
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\complex}{\mathbb{C}}

% Neural network notation
\newcommand{\layer}[1]{^{[#1]}}
\newcommand{\activation}[2]{a\layer{#1}_{#2}}
\newcommand{\weight}[2]{W\layer{#1}_{#2}}
\newcommand{\nbias}[1]{b\layer{#1}}
\newcommand{\cost}[1]{J\left(#1\right)}
\newcommand{\gradient}[2]{\frac{\partial #1}{\partial #2}}

% ============================================================================
% LISTINGS SETUP FOR CODE
% ============================================================================

\lstset{
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{codegray},
    commentstyle=\color{darkgray},
    keywordstyle=\color{primaryblue}\bfseries,
    numberstyle=\tiny\color{darkgray},
    stringstyle=\color{accentorange},
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single,
    rulecolor=\color{lightgray},
    xleftmargin=0.5cm,
    xrightmargin=0.5cm,
    aboveskip=10pt,
    belowskip=10pt
}

% ============================================================================
% HEADER AND FOOTER SETUP
% ============================================================================

\setlength{\headheight}{25pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{primaryblue}{\textbf{Deep Learning Study Notes}}}
\fancyhead[R]{\textcolor{darkgray}{\thepage\ of \pageref{LastPage}}}
\fancyfoot[C]{\textcolor{darkgray}{\small Andrew Ng Course}}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{0.5pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{primaryblue}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrule}{\hbox to\headwidth{\color{lightgray}\leaders\hrule height \footrulewidth\hfill}}

% ============================================================================
% TITLE PAGE SETUP
% ============================================================================

\title{
    \vspace{-1in}
    \begin{flushleft}
    \huge\textbf{\textcolor{primaryblue}{Deep Learning}} \\
    \Large\textcolor{darkgray}{Comprehensive Study Notes} \\
    \vspace{0.2cm}
    \large\textcolor{accentorange}{Andrew Ng Course} \\
    \end{flushleft}
    \vspace{-0.8in}
}

\author{}
\date{}

% ============================================================================
% DOCUMENT SETTINGS
% ============================================================================

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}
\onehalfspacing

% Adjust section spacing
\usepackage{titlesec}
\titleformat{\section}
    {\Large\bfseries\color{primaryblue}}
    {\thesection}
    {1em}
    {}
    [\color{primaryblue}\titlerule]

\titleformat{\subsection}
    {\large\bfseries\color{darkblue}}
    {\thesubsection}
    {1em}
    {}

\titleformat{\subsubsection}
    {\normalsize\bfseries\color{darkgray}}
    {\thesubsubsection}
    {1em}
    {}

% ============================================================================
% DOCUMENT BEGINS
% ============================================================================

\begin{document}

\maketitle

\vspace{0.3cm}

\begin{keyconcept}
\textbf{Welcome to my Deep Learning journey!} \\
This document serves as my study companion for Andrew Ng's Deep Learning course.
\end{keyconcept}

\vspace{0.3cm}

\tableofcontents

\newpage

% ============================================================================
% LOGISTIC REGRESSION
% ============================================================================

\begin{attention}
\textbf{Classical ML vs Deep Learning:} \\
One main difference between "classical" machine learning algorithms and deep learning algorithms is that Deep Learning models "figure out" the best features using the hidden layers.
\end{attention}

\vspace{0.4cm}

\section{Logistic Regression}

We use this learning algorithm when the output learning $y$ in a supervised learning problem are all either zero or one.

\begin{formula}
\textbf{Logistic Regression Prediction:}
\[
\hat{y} = \sigma(\vect{w}^T \vect{x} + b), \quad \text{where} \quad \sigma(z) = \frac{1}{1+e^{-z}}
\]

$\vect{w} \in \reals^{n_x}$, $b \in \reals$ are the neural network \textbf{parameters} 

\textbf{Notation:}

\textbf{Single Training Example:} $(x, y)$ where $x \in \reals^{n_x}$, $y \in \{0,1\}$

\textbf{Training Dataset:} $m$ training examples
\[
\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)})\}
\]

\textbf{Matrix Representation:}
\[
X = \begin{bmatrix}
| & | & & | \\
x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
| & | & & |
\end{bmatrix} \in \reals^{n_x \times m}
\]

\[
Y = \begin{bmatrix} y^{(1)} & y^{(2)} & \cdots & y^{(m)} \end{bmatrix} \in \reals^{1 \times m}
\]

\end{formula}

\vspace{0.4cm}

\begin{intuition}
\textbf{Understanding the Dot Product:} \\
The term $\vect{w}^T \vect{x}$ represents the dot product of the weight vector and input vector:
\[
\vect{w}^T \vect{x} = w_1 x_1 + w_2 x_2 + \cdots + w_{n_x} x_{n_x} = \sum_{i=1}^{n_x} w_i x_i
\]
This gives us a linear combination of the input features, weighted by their importance. The sigmoid function then maps this linear combination to a probability between 0 and 1.
\end{intuition}

\vspace{0.4cm}

\begin{keyconcept}
\textbf{Training Objective:} \\
Given a training set with $m$ examples, we want our predictions to match the true labels:
\[
\hat{y}^{(i)} \approx y^{(i)} \quad \text{for all } i = 1, 2, \ldots, m
\]
where each $y^{(i)} \in \{0, 1\}$ (binary classification).

We need to learn optimal parameters $\vect{w}$ and $b$ that minimize the prediction error across all training examples.
\end{keyconcept}

\vspace{0.4cm}

\begin{followup}
\textbf{How do we measure prediction error?}

\textbf{Key Distinction:}
\begin{itemize}
    \item \textbf{Loss Function} $\mathcal{L}(\hat{y}, y)$: Computes error for a \textbf{single} training example
    \item \textbf{Cost Function} $J(\vect{w}, b)$: Average of loss functions over the \textbf{entire} training set
\end{itemize}

\textbf{Logistic Regression Loss Function:}
\[
\mathcal{L}(\hat{y}, y) = -(y \log \hat{y} + (1-y) \log(1-\hat{y}))
\]

\textbf{Intuition:}
\begin{itemize}
    \item If $y = 1$: $\mathcal{L}(\hat{y}, y) = -\log \hat{y}$ → Want $\hat{y}$ large (close to 1)
    \item If $y = 0$: $\mathcal{L}(\hat{y}, y) = -\log(1-\hat{y})$ → Want $\hat{y}$ small (close to 0)
\end{itemize}

\textbf{Cost Function (Average over all examples):}
\[
J(\vect{w}, b) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}(\hat{y}^{(i)}, y^{(i)})
\]
\end{followup}

\end{document} 